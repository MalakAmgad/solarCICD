name: solar system workflow

on:
  workflow_dispatch:
  push:
    branches: 
      - feature-brancheA
      - main
env: 
   MONGO_URI: "mongodb+srv://supercluster.d83jj.mongodb.net/superData"
   MONGO_USERNAME: ${{ secrets.MONGO_USERNAME }}
   MONGO_PASSWORD: ${{secrets.MONGO_PASSWORD }}


jobs:
  unit-testing:
    name: Unit Testing    
    strategy:
      matrix:
        nodejs_version: [18, 19, 20]   
        os: [ubuntu-latest, macos-latest, windows-latest]
        exclude:
          - nodejs_version: 18
            os: macos-latest
    runs-on: ${{ matrix.os }}

    steps: 
      - name: Checkout Repo
        uses: actions/checkout@v5
      
      - name: Setup NodeJs version - ${{ matrix.nodejs_version }}
        uses: actions/setup-node@v4
        with:
          node-version: ${{ matrix.nodejs_version }}

      - name: Install dep
        run: npm install

      - name: Unit testing
        run: npm test
        id: unit-testing-step  
      
      - name: Archive test result
        if: steps.unit-testing-step.outcome == 'failure' || steps.unit-testing-step.outcome == 'success'
        uses: actions/upload-artifact@v4
        with:
          name: SP-Test-Results-${{matrix.nodejs_version}}-${{matrix.os}}
          path: test-results.xml

  code-coverage:
    name: Code Coverage
    needs: unit-testing
    runs-on: ubuntu-latest
    steps:
        - name: Checkout Repo
          uses: actions/checkout@v2

        - name: Setup Node.js version - 18
          uses: actions/setup-node@v4
          with:
            node-version: 18
        - name: Install dependencies
          run: npm install
        
        - name: Check code coverage
          continue-on-error: true
          run: npm run coverage
        
        - name: Archive coverage results
          uses: actions/upload-artifact@v4
          with:
            name: code-coverage-results
            path: coverage
            retention-days: 5
  
  docker:
      name: containerization
      needs: [code-coverage, unit-testing]
      permissions:
        packages: write
        contents: read
      runs-on: ubuntu-latest
      steps:
        - name: Checkout Repo
          uses: actions/checkout@v5
    
        - name: docker login
          uses: docker/login-action@v2.2.0
          with:
            username: ${{ secrets.DOCKER_HUB_USERNAME }}
            password: ${{ secrets.DOCKER_HUB_TOKEN }}

        - name: GCHR login
          uses: docker/login-action@v2.2.0
          with:
            registry: ghcr.io
            username: ${{ github.actor }}
            password: ${{ secrets.GITHUB_TOKEN }}
    
        - name: Set lowercase repo owner
          id: repo
          run: echo "owner=$(echo '${{ github.repository_owner }}' | tr '[:upper:]' '[:lower:]')" >> $GITHUB_OUTPUT
    
        - name: Build Docker image
          uses: docker/build-push-action@v6
          with:
            push: true
            tags: |
              docker.io/${{ secrets.DOCKER_HUB_USERNAME }}/solar-system:${{ github.sha }}
              docker.io/${{ secrets.DOCKER_HUB_USERNAME }}/solar-system:latest

              ghcr.io/${{ steps.repo.outputs.owner }}/solar-system:${{ github.sha }}
    
    
        - name: Test Docker image
          run: |
            docker images
            docker run --name solar-system-app -d \
            -p 3000:3000 \
            -e MONGO_URI=$MONGO_URI \
            -e MONGO_USERNAME=$MONGO_USERNAME \
            -e MONGO_PASSWORD=$MONGO_PASSWORD \
            ${{ secrets.DOCKER_HUB_USERNAME }}/solar-system:latest
            export IP=$(docker inspect -f '{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' solar-system-app)
            echo "IP Address: $IP"
    
            echo Testing Image URL using wget
            wget -q -O - 127.0.0.1:3000/live | grep live
  trivy-scan:
    name: Trivy Security Scan
    needs: docker
    runs-on: ubuntu-latest
    permissions:        
     contents: read
     security-events: write  
     actions: read
    steps:
      - name: Checkout Repo
        uses: actions/checkout@v5

      - name: Scan Docker Image
        uses: aquasecurity/trivy-action@0.20.0
        with:
          scan-type: image
          image-ref: ${{ secrets.DOCKER_HUB_USERNAME  }}/solar-system:${{ github.sha }}
          format: table
          output: trivy-image-report.txt
          severity: HIGH,CRITICAL
          ignore-unfixed: true
          exit-code: 0

      - name: Generate SBOM (CycloneDX)
        uses: aquasecurity/trivy-action@0.20.0
        with:
          scan-type: image
          image-ref: ${{ secrets.DOCKER_HUB_USERNAME }}/solar-system:${{ github.sha }}
          format: cyclonedx
          output: trivy-sbom.cdx
          exit-code: 0

      - name: Scan Docker Image (SARIF)
        uses: aquasecurity/trivy-action@0.20.0
        with:
          scan-type: image
          image-ref: ${{ secrets.DOCKER_HUB_USERNAME }}/solar-system:${{ github.sha }}
          format: sarif
          output: trivy-results.sarif
          exit-code: 0
      - name: Upload SARIF results
        uses: github/codeql-action/upload-sarif@v3
        with:
          sarif_file: trivy-results.sarif

      - name: Scan Terraform configs
        uses: aquasecurity/trivy-action@0.20.0
        with:
          scan-type: config
          scan-ref: ./terraform
          format: table
          output: trivy-terraform-report.txt
          severity: HIGH,CRITICAL
          ignore-unfixed: true
          exit-code: 0

      - name: Scan Source Code for Secrets
        uses: aquasecurity/trivy-action@0.20.0
        with:
          scan-type: fs
          scan-ref: .
          format: table
          output: trivy-secrets-report.txt
          severity: HIGH,CRITICAL
          exit-code: 0

      - name: Upload Trivy Reports
        if: always()
        uses: actions/upload-artifact@v4.6.2
        with:
          name: trivy-reports
          path: |
            trivy-image-report.txt
            trivy-sbom.cdx
            trivy-results.sarif
            trivy-terraform-report.txt
            trivy-secrets-report.txt
            
  terraform:
    name: Terraform Deployment
    needs: [docker, code-coverage, unit-testing, trivy-scan]
    runs-on: ubuntu-latest
    environment: production
    steps:
      - name: Checkout config files
        uses: actions/checkout@v3

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4.3.1
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: us-west-2

      - name: HashiCorp - Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: "1.1.7"

      - name: Terraform Init
        run: terraform init
        working-directory: ./terraform

      #- name: Terraform Destroy (clean environment)
      #  run: terraform destroy -auto-approve  
      - name: Terraform Plan
        run: terraform plan
        working-directory: ./terraform
        
      - name: Terraform Apply
        run: terraform apply -auto-approve
        working-directory: ./terraform

  deploy:
    needs: terraform
    name: Deploy to EkS
    runs-on: ubuntu-latest

    steps:
      - name: Checkout config files
        uses: actions/checkout@v3

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4.3.1
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: us-west-2

      - name: Update kubeconfig
        run: |
          aws eks --region us-west-2 update-kubeconfig --name stage-eks-cluster

      - name: Trigger app deployment
        uses: statsig-io/kubectl-via-eksctl@main
        env:
           aws_access_key_id: ${{ secrets.AWS_ACCESS_KEY_ID }}
           aws_secret_access_key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
           region: us-west-2
           cluster: stage-eks-cluster
      - name: Deploy k8s deployments
        run: |
         kubectl apply -f deployment.yml
         kubectl apply -f service.yml
        working-directory: ./kubernetes
      - name: Verify Deployment
        run: |
          kubectl get pods  
          kubectl get svc
      - name: Get LoadBalancer URL
        run: |
          echo "Waiting for LoadBalancer to be provisioned..."
          kubectl wait --for=condition=available deployment/microservice-deployment --timeout=120s || true
          kubectl get svc microservice-service -o jsonpath='{.status.loadBalancer.ingress[0].hostname}'
          
  deploy-monitoring:
    needs: deploy
    name: Deploy to EKS Monitor (debug + auto-recover)
    runs-on: ubuntu-latest
    env:
      AWS_REGION: us-west-2
      EKS_CLUSTER_NAME: stage-eks-cluster
      GRAFANA_ADMIN_PASSWORD: ${{ secrets.grafana_admin_password }}
    steps:
      - name: Checkout config files
        uses: actions/checkout@v5
  
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
  
      - name: Update kubeconfig
        run: aws eks --region $AWS_REGION update-kubeconfig --name $EKS_CLUSTER_NAME
  
      - name: Quick cluster checks
        run: |
          kubectl version --short
          echo "Can create cluster roles?"
          kubectl auth can-i create clusterrole || true
          kubectl get ns --no-headers || true
  
      - name: Ensure monitoring namespace
        run: kubectl create namespace monitoring --dry-run=client -o yaml | kubectl apply -f -
  
      - name: Add and update Helm repos
        run: |
          helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
          helm repo add grafana https://grafana.github.io/helm-charts
          helm repo update
  
      - name: Check existing release and uninstall if broken
        run: |
          set -e
          RELEASE=kube-prometheus-stack
          NS=monitoring
          if helm status $RELEASE -n $NS >/dev/null 2>&1; then
            echo "Found existing release - status:"
            helm status $RELEASE -n $NS || true
            echo "History:"
            helm history $RELEASE -n $NS || true
            # If release exists but is not deployed cleanly, uninstall to allow fresh install
            echo "Uninstalling existing release (to recover cleanly)"
            helm uninstall $RELEASE -n $NS || true
          else
            echo "No existing release found"
          fi
  
      - name: Pull chart and apply CRDs (safe)
        working-directory: ${{ github.workspace }}
        run: |
          TMP=/tmp/kube-monitor-chart
          rm -rf $TMP
          mkdir -p $TMP
          helm pull prometheus-community/kube-prometheus-stack --untar --untardir $TMP || true
          # Chart directory name may be kube-prometheus-stack
          CHART_DIR=$(find $TMP -maxdepth 1 -type d -name "kube-prometheus-stack*" | head -n1)
          echo "Chart dir: $CHART_DIR"
          if [ -d "$CHART_DIR/crds" ]; then
            echo "Applying CRDs from chart (may be no-op if already present)"
            kubectl apply -f "$CHART_DIR/crds" || true
          else
            echo "No CRDs folder in chart (helm will handle CRDs)."
          fi
  
      - name: Render values with env
        run: |
          envsubst < values.yml > /tmp/values.rendered.yaml
          tail -n +1 /tmp/values.rendered.yaml
        working-directory: ./kubernetes
  
      - name: Install/Upgrade kube-prometheus-stack (with debug)
        working-directory: ./kubernetes
        run: |
          set -o pipefail
          RELEASE=kube-prometheus-stack
          NS=monitoring
          # Run helm with debug; capture output to file for artifact upload
          helm upgrade --install $RELEASE prometheus-community/kube-prometheus-stack \
            --namespace $NS \
            --values /tmp/values.rendered.yaml \
            --wait \
            --timeout 30m \
            --debug 2>&1 | tee /tmp/helm-install.log
        continue-on-error: true
  
      - name: Collect Kubernetes debug info (always)
        if: always()
        run: |
          mkdir -p /tmp/monitoring-debug
          echo "--- HELM LOG ---" > /tmp/monitoring-debug/README.txt || true
          cp /tmp/helm-install.log /tmp/monitoring-debug/ || true
          echo "=== kubectl get ns ===" >> /tmp/monitoring-debug/README.txt
          kubectl get ns >> /tmp/monitoring-debug/kubectl-ns.txt || true
          echo "=== CRDs (prometheus/grafana) ===" >> /tmp/monitoring-debug/README.txt
          kubectl get crds | grep -E 'prometheus|alertmanager|servicemonitor|grafana' || true > /tmp/monitoring-debug/crds.txt || true
          echo "=== monitoring namespace: pods ===" >> /tmp/monitoring-debug/README.txt
          kubectl -n monitoring get all -o wide > /tmp/monitoring-debug/monitoring-get-all.txt || true
          echo "=== monitoring events (last 200) ===" >> /tmp/monitoring-debug/README.txt
          kubectl -n monitoring get events --sort-by=.metadata.creationTimestamp | tail -n 200 > /tmp/monitoring-debug/events.txt || true
          echo "=== describe pods and logs (last 200 lines) ===" >> /tmp/monitoring-debug/README.txt
          for p in $(kubectl -n monitoring get pods -o name 2>/dev/null || true); do
            echo ">>> $p" >> /tmp/monitoring-debug/pod-describe.txt
            kubectl -n monitoring describe $p >> /tmp/monitoring-debug/pod-describe.txt 2>&1 || true
            kubectl -n monitoring logs --all-containers $p --tail=200 >> /tmp/monitoring-debug/pod-logs.txt 2>&1 || true
          done
          echo "=== kube-system pods (node/infra errors might show there) ===" >> /tmp/monitoring-debug/README.txt
          kubectl -n kube-system get pods -o wide > /tmp/monitoring-debug/kube-system-pods.txt || true
  
      - name: Upload monitoring debug artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: monitoring-debug
          path: /tmp/monitoring-debug
       
  
  
  
  
                            
              
  