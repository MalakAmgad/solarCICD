name: solar system workflow

on:
  workflow_dispatch:
  push:
    branches: 
      - feature-brancheA
      - main
env: 
   MONGO_URI: "mongodb+srv://supercluster.d83jj.mongodb.net/superData"
   MONGO_USERNAME: ${{ secrets.MONGO_USERNAME }}
   MONGO_PASSWORD: ${{secrets.MONGO_PASSWORD }}


jobs:
  unit-testing:
    name: Unit Testing    
    strategy:
      matrix:
        nodejs_version: [18, 19, 20]   
        os: [ubuntu-latest, macos-latest, windows-latest]
        exclude:
          - nodejs_version: 18
            os: macos-latest
    runs-on: ${{ matrix.os }}

    steps: 
      - name: Checkout Repo
        uses: actions/checkout@v5
      
      - name: Setup NodeJs version - ${{ matrix.nodejs_version }}
        uses: actions/setup-node@v4
        with:
          node-version: ${{ matrix.nodejs_version }}

      - name: Install dep
        run: npm install

      - name: Unit testing
        run: npm test
        id: unit-testing-step  
      
      - name: Archive test result
        if: steps.unit-testing-step.outcome == 'failure' || steps.unit-testing-step.outcome == 'success'
        uses: actions/upload-artifact@v4
        with:
          name: SP-Test-Results-${{matrix.nodejs_version}}-${{matrix.os}}
          path: test-results.xml

  code-coverage:
    name: Code Coverage
    needs: unit-testing
    runs-on: ubuntu-latest
    steps:
        - name: Checkout Repo
          uses: actions/checkout@v2

        - name: Setup Node.js version - 18
          uses: actions/setup-node@v4
          with:
            node-version: 18
        - name: Install dependencies
          run: npm install
        
        - name: Check code coverage
          continue-on-error: true
          run: npm run coverage
        
        - name: Archive coverage results
          uses: actions/upload-artifact@v4
          with:
            name: code-coverage-results
            path: coverage
            retention-days: 5
  
  docker:
      name: containerization
      needs: [code-coverage, unit-testing]
      permissions:
        packages: write
        contents: read
      runs-on: ubuntu-latest
      steps:
        - name: Checkout Repo
          uses: actions/checkout@v5
    
        - name: docker login
          uses: docker/login-action@v2.2.0
          with:
            username: ${{ secrets.DOCKER_HUB_USERNAME }}
            password: ${{ secrets.DOCKER_HUB_TOKEN }}

        - name: GCHR login
          uses: docker/login-action@v2.2.0
          with:
            registry: ghcr.io
            username: ${{ github.actor }}
            password: ${{ secrets.GITHUB_TOKEN }}
    
        - name: Set lowercase repo owner
          id: repo
          run: echo "owner=$(echo '${{ github.repository_owner }}' | tr '[:upper:]' '[:lower:]')" >> $GITHUB_OUTPUT
    
        - name: Build Docker image
          uses: docker/build-push-action@v6
          with:
            push: true
            tags: |
              docker.io/${{ secrets.DOCKER_HUB_USERNAME }}/solar-system:${{ github.sha }}
              docker.io/${{ secrets.DOCKER_HUB_USERNAME }}/solar-system:latest

              ghcr.io/${{ steps.repo.outputs.owner }}/solar-system:${{ github.sha }}
    
    
        - name: Test Docker image
          run: |
            docker images
            docker run --name solar-system-app -d \
            -p 3000:3000 \
            -e MONGO_URI=$MONGO_URI \
            -e MONGO_USERNAME=$MONGO_USERNAME \
            -e MONGO_PASSWORD=$MONGO_PASSWORD \
            ${{ secrets.DOCKER_HUB_USERNAME }}/solar-system:latest
            export IP=$(docker inspect -f '{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' solar-system-app)
            echo "IP Address: $IP"
    
            echo Testing Image URL using wget
            wget -q -O - 127.0.0.1:3000/live | grep live
  trivy-scan:
    name: Trivy Security Scan
    needs: docker
    runs-on: ubuntu-latest
    permissions:        
     contents: read
     security-events: write  
     actions: read
    steps:
      - name: Checkout Repo
        uses: actions/checkout@v5

      - name: Scan Docker Image
        uses: aquasecurity/trivy-action@0.20.0
        with:
          scan-type: image
          image-ref: ${{ secrets.DOCKER_HUB_USERNAME  }}/solar-system:${{ github.sha }}
          format: table
          output: trivy-image-report.txt
          severity: HIGH,CRITICAL
          ignore-unfixed: true
          exit-code: 0

      - name: Generate SBOM (CycloneDX)
        uses: aquasecurity/trivy-action@0.20.0
        with:
          scan-type: image
          image-ref: ${{ secrets.DOCKER_HUB_USERNAME }}/solar-system:${{ github.sha }}
          format: cyclonedx
          output: trivy-sbom.cdx
          exit-code: 0

      - name: Scan Docker Image (SARIF)
        uses: aquasecurity/trivy-action@0.20.0
        with:
          scan-type: image
          image-ref: ${{ secrets.DOCKER_HUB_USERNAME }}/solar-system:${{ github.sha }}
          format: sarif
          output: trivy-results.sarif
          exit-code: 0
      - name: Upload SARIF results
        uses: github/codeql-action/upload-sarif@v3
        with:
          sarif_file: trivy-results.sarif

      - name: Scan Terraform configs
        uses: aquasecurity/trivy-action@0.20.0
        with:
          scan-type: config
          scan-ref: ./terraform
          format: table
          output: trivy-terraform-report.txt
          severity: HIGH,CRITICAL
          ignore-unfixed: true
          exit-code: 0

      - name: Scan Source Code for Secrets
        uses: aquasecurity/trivy-action@0.20.0
        with:
          scan-type: fs
          scan-ref: .
          format: table
          output: trivy-secrets-report.txt
          severity: HIGH,CRITICAL
          exit-code: 0

      - name: Upload Trivy Reports
        if: always()
        uses: actions/upload-artifact@v4.6.2
        with:
          name: trivy-reports
          path: |
            trivy-image-report.txt
            trivy-sbom.cdx
            trivy-results.sarif
            trivy-terraform-report.txt
            trivy-secrets-report.txt
            
  terraform:
    name: Terraform Deployment
    needs: [docker, code-coverage, unit-testing, trivy-scan]
    runs-on: ubuntu-latest
    environment: production
    steps:
      - name: Checkout config files
        uses: actions/checkout@v3

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4.3.1
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: us-west-2

      - name: HashiCorp - Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: "1.1.7"

      - name: Terraform Init
        run: terraform init
        working-directory: ./terraform

      #- name: Terraform Destroy (clean environment)
      #  run: terraform destroy -auto-approve  
      - name: Terraform Plan
        run: terraform plan
        working-directory: ./terraform
        
      - name: Terraform Apply
        run: terraform apply -auto-approve
        working-directory: ./terraform

  deploy:
    needs: terraform
    name: Deploy to EkS
    runs-on: ubuntu-latest

    steps:
      - name: Checkout config files
        uses: actions/checkout@v3

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4.3.1
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: us-west-2

      - name: Update kubeconfig
        run: |
          aws eks --region us-west-2 update-kubeconfig --name stage-eks-cluster

      - name: Trigger app deployment
        uses: statsig-io/kubectl-via-eksctl@main
        env:
           aws_access_key_id: ${{ secrets.AWS_ACCESS_KEY_ID }}
           aws_secret_access_key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
           region: us-west-2
           cluster: stage-eks-cluster
      - name: Deploy k8s deployments
        run: |
         kubectl apply -f deployment.yml
         kubectl apply -f service.yml
        working-directory: ./kubernetes
      - name: Verify Deployment
        run: |
          kubectl get pods  
          kubectl get svc
      - name: Get LoadBalancer URL
        run: |
          echo "Waiting for LoadBalancer to be provisioned..."
          kubectl wait --for=condition=available deployment/microservice-deployment --timeout=120s || true
          kubectl get svc microservice-service -o jsonpath='{.status.loadBalancer.ingress[0].hostname}'
          
  deploy-monitoring:
        needs: deploy
        name: deploy Monitoring
        runs-on: ubuntu-latest
        env: 
          AWS_REGION: us-west-2
          EKS_CLUSTER_NAME: stage-eks-cluster
          GRAFANA_ADMIN_PASSWORD: ${{ secrets.grafana_admin_password }}

        steps:
            - name: checkout config files
              uses: actions/checkout@v5


            - name: Configure AWS Credentials
              uses: aws-actions/configure-aws-credentials@v4.3.1
              with:
                  aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
                  aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
                  aws-region: us-west-2

            - name: updat kubeconfig
              run: | 
                    aws eks --region us-west-2 update-kubeconfig --name stage-eks-cluster

            - name: Trigger app deployment
              uses: statsig-io/kubectl-via-eksctl@main
              env:
                 aws_access_key_id: ${{ secrets.AWS_ACCESS_KEY_ID }}
                 aws_secret_access_key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
                 region: us-west-2
                 cluster: stage-eks-cluster
            
            - name: create monitoring namespace
              run: kubectl create namespace monitoring --dry-run=client -o yaml | kubectl apply -f -

            - name: Add Helm repos
              run: |
                helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
                helm repo add grafana https://grafana.github.io/helm-charts
                helm repo add autoscaler https://kubernetes.github.io/autoscaler
                helm repo update
                
                helm upgrade --install cluster-autoscaler autoscaler/cluster-autoscaler \
                  --namespace kube-system \
                  --set autoDiscovery.clusterName=stage-eks-cluster \
                  --set autoDiscovery.enabled=true \
                  --set awsRegion=us-west-2
                kubectl get pods -n kube-system | grep cluster-autoscaler
                kubectl logs -n kube-system $(kubectl get pods -n kube-system \ 
                    -l app.kubernetes.io/name=aws-cluster-autoscaler -o jsonpath='{.items[0].metadata.name}')

  

            - name: Render values with env
              run: |
               envsubst < values.yml > /tmp/values.rendered.yaml
               echo "Rendered values:"
               tail -n +1 /tmp/values.rendered.yaml
              working-directory: ./kubernetes

            - name: Install/Upgrade kube-prometheus-stack
              run: |
                helm upgrade --install kube-prometheus-stack prometheus-community/kube-prometheus-stack \
                  --namespace monitoring \
                  --values /tmp/values.rendered.yaml \
                  --wait --timeout 2m 
            - name: Debug monitoring namespace
              if: failure()
              run: |
                echo "==== Pods ===="
                kubectl get pods -n monitoring
                echo "==== Events ===="
                kubectl get events -n monitoring --sort-by=.lastTimestamp | tail -n 30
                echo "==== CRDs ===="
                kubectl get crd | grep monitoring.coreos.com
                kubectl get pods -A | grep grafana
                kubectl get pods -A | grep prometheus

      
                  
            - name: Apply ServiceMonitor for my app
              run: |
                kubectl apply -f servicemonitor-solar-system.yaml
              working-directory: ./kubernetes

            - name: Show external endpoints
              run: |
                kubectl -n monitoring wait --for=condition=available deploy/kube-prometheus-stack-grafana  --timeout=10m
                kubectl -n monitoring get svc -o wide
                echo "Grafana URL:"
                kubectl -n monitoring get svc kube-prometheus-stack-grafana -o jsonpath='{.status.loadBalancer.ingress[0].hostname}{"\n"}{.status.loadBalancer.ingress[0].ip}{"\n"}'
                echo "Prometheus URL:"
                kubectl -n monitoring get svc kube-prometheus-stack-prometheus -o jsonpath='{.status.loadBalancer.ingress[0].hostname}{"\n"}{.status.loadBalancer.ingress[0].ip}{"\n"}'

                    
       
  
  
  
  
                            
              
  